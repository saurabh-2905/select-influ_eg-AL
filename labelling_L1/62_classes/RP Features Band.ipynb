{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Requirements:\n",
    "\n",
    "```\n",
    "pip install tensorflow-gpu==2.0\n",
    "pip install numpy\n",
    "pip install scikit-learn\n",
    "pip install PIL\n",
    "pip install scipy\n",
    "pip install tqdm\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm_notebook\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from inspect import signature\n",
    "from PIL import Image\n",
    "from scipy.cluster.vq import kmeans\n",
    "from multiprocessing import Pool\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usefull functions used for this experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# following the implementation of soft voting scheme\n",
    "# see paper: https://arxiv.org/abs/1512.05227\n",
    "\n",
    "def soft_distance(x, features, gamma):\n",
    "    diff = x - features          \n",
    "    dist = np.sqrt(np.square(diff)).sum(axis=-1)\n",
    "    return np.exp(-gamma * dist).sum()\n",
    "\n",
    "def calculate_distances(features, anchor, gamma):\n",
    "    labels = np.zeros((features.shape[0],))\n",
    "    for i, feature in enumerate(features): \n",
    "        distances = np.zeros((len(unique_classes),))\n",
    "        for cid in unique_classes:            \n",
    "            n = soft_distance(feature, anchor[cid], gamma)            \n",
    "            ds = EPS\n",
    "            for ccid in unique_classes:                \n",
    "                d = soft_distance(feature, anchor[ccid], gamma)\n",
    "                ds += d\n",
    "            \n",
    "            distances[cid] = n/ds\n",
    "            \n",
    "        labels[i] = distances.argmax()\n",
    "    return labels\n",
    "\n",
    "def get_soft_voting(features, anchor, gamma=5.0):       \n",
    "    labels = []\n",
    "    cpus = cpu_count() - 2 # keep two cpus please\n",
    "    num_chunks = features.shape[0] // (cpus - 1)\n",
    "    \n",
    "    with Pool(processes=cpus) as pool:\n",
    "        results = []\n",
    "\n",
    "        for chunk in range(cpus):\n",
    "            start = chunk * num_chunks\n",
    "            \n",
    "            if chunk == cpus - 1:\n",
    "                sub_features = features[start:]\n",
    "            else:\n",
    "                sub_features = features[start:start + num_chunks]\n",
    "            \n",
    "            results.append(pool.apply_async(calculate_distances, args=(sub_features, anchor, gamma)))\n",
    "        \n",
    "        with tqdm_notebook(total=features.shape[0]) as pbar:        \n",
    "            for result in results:\n",
    "                res = result.get()\n",
    "                labels.extend(res)\n",
    "                pbar.update(len(res))\n",
    "        \n",
    "    return labels\n",
    "\n",
    "\n",
    "def get_anchor_points(num_points):\n",
    "    anchor_points = []\n",
    "    for cid in tqdm_notebook(unique_classes, desc='calculate anchor points'):\n",
    "        indices = np.where(reference_classes == cid)[0]\n",
    "        class_features = reference_features[indices]\n",
    "        codebook, _ = kmeans(class_features, num_points, iter=20)\n",
    "        anchor_points += [codebook]\n",
    "    \n",
    "    return np.array(anchor_points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### variable definition\n",
    "\n",
    "- *_features containing all feature vectors of 2048 dimensions\n",
    "- *_classes contains the coressponding class ids to the features\n",
    "- *_probs contains the softmax probabilities\n",
    "- class_mapping contains the class name as key and the id as value (actually only used for visualization purposes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/tschec/notebooks/Paper/Personalized_CNN\\\\training_features_62_classes.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-0a01da6e58fb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mlogs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'logs/'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mreference_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_to_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'training_features_62_classes.npy'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[0mreference_classes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_to_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'training_classids_62_classes.npy'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mreference_probs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_to_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'training_probabilities_62_classes.npy'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python\\python3\\lib\\site-packages\\numpy\\lib\\npyio.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[0;32m    426\u001b[0m         \u001b[0mown_fid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    427\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 428\u001b[1;33m         \u001b[0mfid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos_fspath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    429\u001b[0m         \u001b[0mown_fid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    430\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/tschec/notebooks/Paper/Personalized_CNN\\\\training_features_62_classes.npy'"
     ]
    }
   ],
   "source": [
    "# folder where all *.npy files are located\n",
    "path_to_data = '/home/tschec/notebooks/Paper/Personalized_CNN'\n",
    "\n",
    "# create log files in `current dir/`\n",
    "logs = 'logs/' \n",
    "\n",
    "reference_features = np.load(os.path.join(path_to_data, 'training_features_62_classes.npy'))\n",
    "reference_classes = np.load(os.path.join(path_to_data, 'training_classids_62_classes.npy'))\n",
    "reference_probs = np.load(os.path.join(path_to_data, 'training_probabilities_62_classes.npy'))\n",
    "\n",
    "incremental_features = np.load(os.path.join(path_to_data, 'validation_features_62_classes.npy'))\n",
    "incremental_classes = np.load(os.path.join(path_to_data, 'validation_classids_62_classes.npy'))\n",
    "incremental_probs = np.load(os.path.join(path_to_data, 'validation_probabilities_62_classes.npy'))\n",
    "\n",
    "test_features = np.load(os.path.join(path_to_data, 'test_features_62_classes.npy'))\n",
    "test_classes = np.load(os.path.join(path_to_data, 'test_classids_62_classes.npy'))\n",
    "test_probs = np.load(os.path.join(path_to_data, 'test_probabilities_62_classes.npy'))\n",
    "\n",
    "class_mapping = np.load(os.path.join(path_to_data, 'class_mapping_62_classes.npy'), allow_pickle=True).item()\n",
    "unique_classes = np.unique(reference_classes)\n",
    "\n",
    "# create tf.data datasets for training based on our features and labels\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((test_features, test_classes))\n",
    "test_dataset = test_dataset.batch(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate an array of shape (62,10,2048) = 10 anchor points for 62 classes with 2048 dimensions\n",
    "anchors = get_anchor_points(10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_weights(features, name):\n",
    "    if name == 'baseline':\n",
    "        return np.ones(features.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do some fancy magic to calculate the weights\n",
    "# but for now we use 1.0 (float is important)\n",
    "class_weights = {}\n",
    "for class_id in unique_classes:\n",
    "    class_weights[class_id] = 1.0\n",
    "\n",
    "# this dictionary contains an experiment name and the corresponding weights\n",
    "# based on this values the experiments are exectued and logged into path/to/log/dir/expermentname\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### run experiment\n",
    "- run the experiment for every weight listed in `experiment_weights`\n",
    "- load with every experiment iteration the pre-trained model and train it with the new weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'experiment_weights' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-75fa2604768f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mexperiment_weights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m.0001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'categorical_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     callbacks = [\n\u001b[1;32m      5\u001b[0m         ModelCheckpoint(\n",
      "\u001b[0;31mNameError\u001b[0m: name 'experiment_weights' is not defined"
     ]
    }
   ],
   "source": [
    "experiment_weights_types = ['baseline', '.9 Threshold']\n",
    "for name in experiment_weights_types:\n",
    "    # sample weights \n",
    "    weights = calculate_weights(incremental_features, name)\n",
    "    \n",
    "    \n",
    "    incremental_dataset = tf.data.Dataset.from_tensor_slices((incremental_features, incremental_classes, sample_weights))\n",
    "    incremental_dataset = incremental_dataset.shuffle(2048).batch(32)\n",
    "    # load pre-trained keras model\n",
    "    model = tf.keras.models.load_model(os.path.join(path_to_data, 'submodel_62_classes.hdf5'))\n",
    "\n",
    "    # use SGD optimizer\n",
    "    optimizer = SGD(lr=.0001, momentum=0.9)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "    # define callbacks \n",
    "    callbacks = [\n",
    "        ModelCheckpoint(\n",
    "            os.path.join(logs, '{}_model_62_classes.hdf5'.format(name)), monitor='val_accuracy', \n",
    "            verbose=0, save_best_only=True, save_weights_only=False, mode='auto'),\n",
    "        # you can use tensorboard --logdir path/to/log/dir to visualize the training parameters like loss and accuracy\n",
    "        TensorBoard(os.path.join(logs, name))\n",
    "    ] \n",
    "\n",
    "    # start training for 250 epochs\n",
    "    model.fit_generator(\n",
    "        incremental_dataset,\n",
    "        epochs=250,\n",
    "        callbacks=callbacks,\n",
    "        class_weights=weights,\n",
    "        validation_data=test_dataset)\n",
    "    \n",
    "    y_pred = model.predict(test_dataset)\n",
    "    \n",
    "    cm = confusion_matrix(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
